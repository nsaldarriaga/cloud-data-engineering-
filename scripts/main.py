"""
Script principal para recolecci√≥n de datos meteorol√≥gicos.

Ejecuta la recolecci√≥n de datos hist√≥ricos y/o de pron√≥stico para
todas las ubicaciones configuradas, con validaci√≥n y manejo de errores.
"""

import argparse
import logging
from pathlib import Path
from typing import List, Optional
import pandas as pd

# Imports del paquete weather_data_collector
from weather_data_collector.api_client import WeatherAPIClient
from weather_data_collector.config import WeatherConfig, Location
from weather_data_collector.utils import DataUtils

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WeatherDataPipeline:
    """
    Pipeline principal para la recolecci√≥n de datos meteorol√≥gicos.
    
    Coordina la obtenci√≥n, validaci√≥n y almacenamiento de datos
    desde las APIs de OpenMeteo.
    """
    
    def __init__(self, config: Optional[WeatherConfig] = None):
        """
        Inicializa el pipeline.
        
        Args:
            config: Configuraci√≥n a usar. Si no se proporciona, usa la por defecto.
        """
        self.config = config or WeatherConfig()
        self.api_client = WeatherAPIClient(self.config)
        self.data_utils = DataUtils()
        
        # Crear directorio de salida
        Path(self.config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
        
        logger.info("üöÄ Pipeline inicializado")
    
    def process_location_historical(self, location: Location) -> bool:
        """
        Procesa datos hist√≥ricos para una ubicaci√≥n espec√≠fica.
        
        Args:
            location: Ubicaci√≥n a procesar
            
        Returns:
            bool: True si se proces√≥ correctamente
        """
        logger.info(f"üìà Procesando datos hist√≥ricos para {location.name}")
        
        try:
            # Obtener datos
            df = self.api_client.fetch_historical_data(location)
            
            if df.empty:
                logger.warning(f"‚ö†Ô∏è No se obtuvieron datos hist√≥ricos para {location.name}")
                return False
            
            # Validar calidad de datos
            quality_report = self.data_utils.validate_data_quality(df, location.name)
            if quality_report["status"] == "error":
                logger.error(f"‚ùå Datos hist√≥ricos inv√°lidos para {location.name}")
                return False
            
            # Guardar archivo
            filename = f"historical_{location.name}_{self.config.HISTORICAL_END_DATE}.json"
            output_path = Path(self.config.OUTPUT_DIR) / filename
            
            success = self.data_utils.save_to_json(df, output_path)
            
            if success:
                logger.info(f"‚úÖ Datos hist√≥ricos guardados para {location.name}")
                return True
            else:
                logger.error(f"‚ùå Error guardando datos hist√≥ricos para {location.name}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error procesando hist√≥ricos para {location.name}: {e}")
            return False
    
    def process_location_forecast(self, location: Location) -> bool:
        """
        Procesa datos de pron√≥stico para una ubicaci√≥n espec√≠fica.
        
        Args:
            location: Ubicaci√≥n a procesar
            
        Returns:
            bool: True si se proces√≥ correctamente
        """
        logger.info(f"üîÆ Procesando pron√≥stico para {location.name}")
        
        try:
            # Obtener datos
            df = self.api_client.fetch_forecast_data(location)
            
            if df.empty:
                logger.warning(f"‚ö†Ô∏è No se obtuvo pron√≥stico para {location.name}")
                return False
            
            # Validar calidad de datos
            quality_report = self.data_utils.validate_data_quality(df, location.name)
            if quality_report["status"] == "error":
                logger.error(f"‚ùå Datos de pron√≥stico inv√°lidos para {location.name}")
                return False
            
            # Guardar archivo
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d")
            filename = f"forecast_{location.name}_{timestamp}.json"
            output_path = Path(self.config.OUTPUT_DIR) / filename
            
            success = self.data_utils.save_to_json(df, output_path)
            
            if success:
                logger.info(f"‚úÖ Pron√≥stico guardado para {location.name}")
                return True
            else:
                logger.error(f"‚ùå Error guardando pron√≥stico para {location.name}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error procesando pron√≥stico para {location.name}: {e}")
            return False
    
    def merge_datasets(self, location: Location, 
                      df_historical: pd.DataFrame, 
                      df_forecast: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        Combina datasets hist√≥rico y de pron√≥stico de forma inteligente.
        
        Args:
            location: Ubicaci√≥n correspondiente
            df_historical: Datos hist√≥ricos
            df_forecast: Datos de pron√≥stico
            
        Returns:
            pd.DataFrame or None: Dataset combinado o None si hay error
        """
        try:
            if df_historical.empty or df_forecast.empty:
                logger.warning(f"‚ö†Ô∏è No se puede combinar datos vac√≠os para {location.name}")
                return None
            
            logger.info(f"üîÑ Combinando datasets para {location.name}")
            
            # Asegurar que las fechas sean datetime
            df_historical['date'] = pd.to_datetime(df_historical['date'])
            df_forecast['date'] = pd.to_datetime(df_forecast['date'])
            
            # Combinar datos
            df_combined = pd.concat([df_historical, df_forecast], ignore_index=True)
            
            # Eliminar duplicados (priorizando datos m√°s recientes - forecast)
            df_combined = df_combined.sort_values(['date', 'location'])
            df_combined = df_combined.drop_duplicates(
                subset=['date', 'location'], 
                keep='last'  # Mantener el m√°s reciente
            )
            
            # Ordenar por fecha
            df_combined = df_combined.sort_values('date').reset_index(drop=True)
            
            logger.info(f"‚úÖ Datasets combinados: {len(df_combined)} registros totales")
            
            return df_combined
            
        except Exception as e:
            logger.error(f"‚ùå Error combinando datasets para {location.name}: {e}")
            return None
    
    def run_pipeline(self, 
                include_historical: bool = True,
                include_forecast: bool = True,
                create_combined: bool = True,
                locations: Optional[List[str]] = None) -> dict[str, bool]:
        """
        Ejecuta el pipeline completo de recolecci√≥n de datos.
        
        Args:
            include_historical: Si incluir datos hist√≥ricos
            include_forecast: Si incluir datos de pron√≥stico
            create_combined: Si crear archivos combinados
            locations: Lista de nombres de ubicaciones espec√≠ficas (None = todas)
            
        Returns:
            dict: Reporte de √©xito/fallo por ubicaci√≥n
        """
        logger.info("üåç Iniciando pipeline de recolecci√≥n de datos")
        
        # Filtrar ubicaciones si se especificaron
        locations_to_process = self.config.LOCATIONS
        if locations:
            locations_to_process = [
                loc for loc in self.config.LOCATIONS 
                if loc.name in locations
            ]
            
        if not locations_to_process:
            logger.error("‚ùå No hay ubicaciones v√°lidas para procesar")
            return {}
        
        results = {}
        
        for location in locations_to_process:
            logger.info(f"üìç Procesando ubicaci√≥n: {location.name}")
            
            location_results = {
                'historical': False,
                'forecast': False,
                'combined': False
            }
            
            df_historical = pd.DataFrame()
            df_forecast = pd.DataFrame()
            
            # Procesar datos hist√≥ricos
            if include_historical:
                if self.process_location_historical(location):
                    location_results['historical'] = True
                    df_historical = self.api_client.fetch_historical_data(location)
            
            # Procesar datos de pron√≥stico
            if include_forecast:
                if self.process_location_forecast(location):
                    location_results['forecast'] = True
                    df_forecast = self.api_client.fetch_forecast_data(location)
            
            # Crear dataset combinado
            if (create_combined and 
                location_results['historical'] and 
                location_results['forecast']):
                
                df_combined = self.merge_datasets(location, df_historical, df_forecast)
                
                if df_combined is not None:
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d")
                    filename = f"combined_{location.name}_{timestamp}.json"
                    output_path = Path(self.config.OUTPUT_DIR) / filename
                    
                    if self.data_utils.save_to_json(df_combined, output_path):
                        location_results['combined'] = True
                        logger.info(f"‚úÖ Dataset combinado creado para {location.name}")
            
            results[location.name] = location_results
        
        # Reporte final
        self._print_final_report(results)
        
        return results
    
    def _print_final_report(self, results: dict[str, dict[str, bool]]) -> None:
        """
        Imprime reporte final del pipeline.
        
        Args:
            results: Resultados del pipeline por ubicaci√≥n
        """
        logger.info("\n" + "="*60)
        logger.info("üìä REPORTE FINAL DEL PIPELINE")
        logger.info("="*60)
        
        total_locations = len(results)
        successful_locations = 0
        
        for location_name, location_results in results.items():
            success_count = sum(location_results.values())
            total_processes = len(location_results)
            
            if success_count > 0:
                successful_locations += 1
            
            status = "‚úÖ" if success_count == total_processes else "‚ö†Ô∏è" if success_count > 0 else "‚ùå"
            
            logger.info(f"{status} {location_name}: {success_count}/{total_processes} procesos exitosos")
            
            for process, success in location_results.items():
                icon = "‚úÖ" if success else "‚ùå"
                logger.info(f"   {icon} {process}")
        
        logger.info("-" * 60)
        logger.info(f"üèÅ Resumen: {successful_locations}/{total_locations} ubicaciones procesadas")
        logger.info("="*60)

def main():
    """Funci√≥n principal con argumentos de l√≠nea de comandos."""
    parser = argparse.ArgumentParser(description="Recolector de datos meteorol√≥gicos")
    
    parser.add_argument(
        "--skip-historical", 
        action="store_true", 
        help="Omitir recolecci√≥n de datos hist√≥ricos"
    )
    parser.add_argument(
        "--skip-forecast", 
        action="store_true", 
        help="Omitir recolecci√≥n de pron√≥sticos"
    )
    parser.add_argument(
        "--no-combined", 
        action="store_true", 
        help="No crear archivos combinados"
    )
    parser.add_argument(
        "--locations", 
        nargs="+", 
        help="Ubicaciones espec√≠ficas a procesar (ej: iowa_center illinois_center)"
    )
    parser.add_argument(
        "--log-level", 
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
        default='INFO',
        help="Nivel de logging"
    )
    
    args = parser.parse_args()
    
    # Configurar nivel de logging
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    try:
        # Inicializar pipeline
        pipeline = WeatherDataPipeline()
        
        # Ejecutar pipeline
        results = pipeline.run_pipeline(
            include_historical=not args.skip_historical,
            include_forecast=not args.skip_forecast,
            create_combined=not args.no_combined,
            locations=args.locations
        )
        
        # Verificar si hubo alg√∫n √©xito
        any_success = any(
            any(location_results.values()) 
            for location_results in results.values()
        )
        
        if any_success:
            logger.info("üéâ Pipeline completado con √©xito")
            exit(0)
        else:
            logger.error("üí• Pipeline fall√≥ completamente")
            exit(1)
            
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Pipeline interrumpido por el usuario")
        exit(1)
    except Exception as e:
        logger.error(f"üí• Error cr√≠tico en el pipeline: {e}")
        exit(1)

if __name__ == "__main__":
    main()